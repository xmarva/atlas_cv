{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10510129,"sourceType":"datasetVersion","datasetId":6476878}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hi Team Atlas!\n\nI thought the easiest way to handle this would be to share the notebook on Kaggle, so it’s super simple to run and check that everything works.\n\nPlease make sure to run the notebook with GPU enabled right away, since I used Segment Anything for the second task (yeah, I know it’s overkill, but that’s how I started, and then I didn’t feel like reworking it).\n\nThe task you said was the easiest actually turned out to be the most annoying and tedious for me! I used Breadth-First Search for it, and, well… it is what it is.\n\nBut I really enjoyed task 4 the most. Even though I had to wrack my brain a bit, this was the best I could come up with. Overall, the task instructions weren’t super strict, so I figured that whatever wasn’t explicitly forbidden was fair game.","metadata":{}},{"cell_type":"markdown","source":"## Task 4","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom skimage.measure import label, regionprops\nfrom skimage.morphology import remove_small_objects\nfrom skimage.util import invert\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef preprocess_image(image_path):\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    image = invert(img)\n    \n    sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n    sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n    magnitude = np.sqrt(sobelx**2 + sobely**2)\n    magnitude = np.uint8(magnitude * 255 / np.max(magnitude))\n    \n    _, edges_thresh = cv2.threshold(magnitude, 8, 255, cv2.THRESH_BINARY)\n    _, thresh = cv2.threshold(magnitude, 0, 255, cv2.THRESH_BINARY)\n    \n    binary = thresh > 0\n    filled = remove_small_objects(~binary, min_size=1500)\n    result_mask = ~filled\n    \n    edges = edges_thresh > 0\n    \n    return img, edges, result_mask\n\ndef get_shape_features(region):\n    moments = cv2.moments(region.image.astype(np.uint8))\n    hu_moments = cv2.HuMoments(moments).flatten()\n    hu_moments = -np.sign(hu_moments) * np.log10(np.abs(hu_moments))\n    \n    features = [\n        region.eccentricity,  \n        region.solidity,     \n        region.extent, \n    ]\n    \n    return np.concatenate([hu_moments, features])\n\ndef find_and_analyze_contours(edges_image, masks_image):\n    edges = edges_image.astype(np.uint8) * 255\n    masks = masks_image.astype(np.uint8) * 255\n    \n    contours, hierarchy = cv2.findContours(edges, cv2.RETR_TREE, \n                                         cv2.CHAIN_APPROX_SIMPLE)\n    \n    contour_image = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n    cv2.drawContours(contour_image, contours, -1, (0,0,255), 2)\n    \n    result_mask = masks.copy()\n    \n    for i, contour in enumerate(contours):\n        if cv2.contourArea(contour) < 100:\n            continue\n            \n        mask = np.zeros_like(edges)\n        cv2.drawContours(mask, [contour], -1, 255, -1)\n        \n        mask_pixels = masks[mask == 255]\n        \n        if len(mask_pixels) > 0:\n            white_percentage = np.sum(mask_pixels == 255) / len(mask_pixels)\n            if white_percentage > 0.5:\n                result_mask[mask == 255] = 255\n                \n        M = cv2.moments(contour)\n        if M['m00'] != 0:\n            cx = int(M['m10']/M['m00'])\n            cy = int(M['m01']/M['m00'])\n            cv2.putText(contour_image, str(i), (cx, cy),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n    \n    print(f\"Contours: {len(contours)}\")\n    \n    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n    \n    axs[0,0].imshow(edges_image, cmap='gray')\n    axs[0,0].set_title('Raw Edges')\n    axs[0,0].axis('off') \n    \n    axs[0,1].imshow(masks_image, cmap='gray')\n    axs[0,1].set_title('Raw Masks')\n    axs[0,1].axis('off') \n    \n    axs[1,0].imshow(cv2.cvtColor(contour_image, cv2.COLOR_BGR2RGB))\n    axs[1,0].set_title('Edges')\n    axs[1,0].axis('off') \n    \n    axs[1,1].imshow(result_mask, cmap='gray')\n    axs[1,1].set_title('Masks')\n    axs[1,1].axis('off') \n    \n    plt.axis('off') \n    plt.show()\n    \n    return contour_image, result_mask\n\ndef separate_large_regions(mask, min_area=1500):\n    kernel = np.ones((8,8), np.uint8)\n    eroded = cv2.erode(mask.astype(np.uint8), kernel, iterations=2)\n    dilated = cv2.dilate(eroded, kernel, iterations=2)\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(dilated, cmap='gray')\n    plt.title('Large ROI mask')\n    plt.axis('off') \n    plt.show()\n    \n    labeled = label(dilated)\n    regions = regionprops(labeled)\n    large_regions = [region for region in regions if region.area >= min_area]\n    large_regions.sort(key=lambda x: x.area, reverse=True)\n    \n    return large_regions, labeled\n\ndef classify_shapes(regions):\n    if len(regions) < 2:\n        return [0] * len(regions)\n    \n    features = np.array([get_shape_features(region) for region in regions])\n    features = (features - features.mean(axis=0)) / (features.std(axis=0) + 1e-10)\n    \n    kmeans = KMeans(n_clusters=3, random_state=42)\n    labels = kmeans.fit_predict(features)\n    \n    return labels\n\ndef visualize_classification(original_img, mask, regions, labeled, shape_labels):\n    colored_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n    original_with_overlay = cv2.cvtColor(mask.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n    final_overlay = cv2.cvtColor(original_img, cv2.COLOR_GRAY2BGR)\n    \n    colors_rgb = [\n        [255, 30, 191],\n        [200, 85, 221],\n        [138, 18, 229]\n    ]\n    \n    colors_bgr = [[color[2], color[1], color[0]] for color in colors_rgb]\n    \n    class_masks = [np.zeros_like(mask) for _ in range(3)]\n    \n    for region, shape_class in zip(regions[:12], shape_labels[:12]):\n        color_bgr = colors_bgr[shape_class]\n        region_mask = labeled == region.label\n        \n        class_masks[shape_class][region_mask] = 1\n        \n        colored_mask[region_mask] = color_bgr\n        original_with_overlay[region_mask] = color_bgr\n        final_overlay[region_mask] = color_bgr\n    \n    plt.figure(figsize=(15, 10))\n    \n    plt.subplot(221)\n    plt.imshow(cv2.cvtColor(colored_mask, cv2.COLOR_BGR2RGB))\n    plt.axis('off') \n    plt.title('Classification results')\n    \n    plt.subplot(222)\n    plt.imshow(cv2.cvtColor(original_with_overlay, cv2.COLOR_BGR2RGB))\n    plt.axis('off') \n    plt.title('Classification with mask')\n    \n    fig, axs = plt.subplots(1, len(class_masks), figsize=(20, 8))\n    \n    for i, ax in enumerate(axs):\n        class_visualization = np.zeros((*mask.shape, 3), dtype=np.uint8)\n        class_visualization[class_masks[i] == 1] = colors_bgr[i]\n        \n        ax.imshow(cv2.cvtColor(class_visualization, cv2.COLOR_BGR2RGB))\n        ax.set_title(f'Class {i+1} object mask')\n        ax.axis('off') \n    \n    plt.tight_layout()\n    plt.show()\n\n    final_overlay = cv2.cvtColor(original_img, cv2.COLOR_GRAY2BGR)\n    \n    overlay_color = np.zeros_like(final_overlay, dtype=np.uint8)\n    for i in range(len(class_masks)):\n        overlay_color[class_masks[i] == 1] = colors_bgr[i]\n    \n    final_overlay = cv2.addWeighted(final_overlay, 1, overlay_color, 0.7, 0)\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(cv2.cvtColor(final_overlay, cv2.COLOR_BGR2RGB))\n    plt.title('Masked Image')\n    plt.axis('off')\n    plt.show()\n\n\ndef process_image(image_path):\n    original_img, edges, result_mask = preprocess_image(image_path)\n    contour_image, result_mask = find_and_analyze_contours(edges, result_mask)\n    \n    large_regions, labeled_mask = separate_large_regions(result_mask)\n    shape_labels = classify_shapes(large_regions[:12])\n    \n    visualize_classification(original_img, result_mask, large_regions, labeled_mask, shape_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T00:18:12.035886Z","iopub.execute_input":"2025-01-21T00:18:12.036103Z","iopub.status.idle":"2025-01-21T00:18:14.400178Z","shell.execute_reply.started":"2025-01-21T00:18:12.036075Z","shell.execute_reply":"2025-01-21T00:18:14.399531Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = \"/kaggle/input/3d-singleshot/4.png\"\nprocess_image(image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T00:18:14.401105Z","iopub.execute_input":"2025-01-21T00:18:14.401393Z","iopub.status.idle":"2025-01-21T00:18:17.007535Z","shell.execute_reply.started":"2025-01-21T00:18:14.401373Z","shell.execute_reply":"2025-01-21T00:18:17.006654Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Task 3","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance\nfrom PIL import Image\n\ndef preprocess_image(image):\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = image\n    _, binary = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY)\n    return binary\n\ndef filter_hemispheres(binary_image, visualization=True):\n    contours, _ = cv2.findContours(binary_image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    filtered_mask = np.zeros_like(binary_image)\n    hemisphere_mask = np.zeros_like(binary_image)\n    \n    areas = [cv2.contourArea(c) for c in contours]\n    median_area = np.median(areas)\n    \n    for contour in contours:\n        area = cv2.contourArea(contour)\n        perimeter = cv2.arcLength(contour, True)\n        \n        circularity = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0\n        \n        x, y, w, h = cv2.boundingRect(contour)\n        aspect_ratio = w / h if h > 0 else 0\n        \n        if (0.01 < area / median_area < 7 and \n            circularity > 0.01 and \n            0.01 < aspect_ratio < 7):\n            cv2.drawContours(hemisphere_mask, [contour], -1, 255, -1)\n        else:\n            cv2.drawContours(filtered_mask, [contour], -1, 255, -1)\n    \n    if visualization:\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n        axes[0].imshow(binary_image, cmap='gray')\n        axes[0].set_title('Original Binary')\n        axes[1].imshow(hemisphere_mask, cmap='gray')\n        axes[1].set_title('Detected Monkeys')\n        axes[2].imshow(filtered_mask, cmap='gray')\n        axes[2].set_title('Noise Only')\n        plt.tight_layout()\n        plt.show()\n    \n    return hemisphere_mask\n\ndef cluster_shapes(binary_mask, eps=50, min_samples=3, visualization=True):\n    contours, _ = cv2.findContours(binary_mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    centroids = []\n    for contour in contours:\n        M = cv2.moments(contour)\n        if M[\"m00\"] != 0:\n            cx = int(M[\"m10\"] / M[\"m00\"])\n            cy = int(M[\"m01\"] / M[\"m00\"])\n            centroids.append([cx, cy])\n    \n    if not centroids:\n        return np.zeros_like(binary_mask)\n    \n    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(centroids)\n    labels = clustering.labels_\n    \n    result = np.zeros_like(binary_mask)\n    colors = [(255, 0, 0), (0, 255, 0)] \n    \n    if visualization:\n        plt.figure(figsize=(10, 10))\n        plt.imshow(binary_mask, cmap='gray')\n        \n        # Plot centroids and clusters\n        centroids = np.array(centroids)\n        unique_labels = set(labels)\n        for label_idx, label in enumerate(unique_labels):\n            if label == -1:\n                continue\n            mask = labels == label\n            plt.scatter(centroids[mask, 0], centroids[mask, 1], \n                       c=[f'C{label_idx}'], label=f'Cluster {label}')\n            \n            # convex hull around cluster points\n            cluster_points = centroids[mask].astype(np.int32)\n            if len(cluster_points) >= 3: \n                hull = cv2.convexHull(cluster_points)\n                plt.fill(hull[:, 0, 0], hull[:, 0, 1], alpha=0.2, c=f'C{label_idx}')\n        \n        plt.legend()\n        plt.title('Clustered Monkeys')\n        plt.show()\n    \n    return result\n\ndef create_chain_mask(image, binary_mask, centroids, labels, visualization=True):\n    height, width = binary_mask.shape\n    elegant_red = (138, 18, 229)  \n    mask_rgb = np.zeros((height, width, 3), dtype=np.uint8)\n    \n    centroids = np.array(centroids)\n    if len(centroids) == 0:\n        return mask_rgb\n    \n    unique_labels = sorted(set(labels) - {-1})\n    if not unique_labels:\n        return mask_rgb\n    \n    cluster_sizes = [(label, np.sum(labels == label)) for label in unique_labels]\n    two_largest = sorted(cluster_sizes, key=lambda x: x[1], reverse=True)[:2]\n    largest_labels = [label for label, _ in two_largest]\n    \n    for label in largest_labels:\n        mask = labels == label\n        cluster_points = centroids[mask]\n        \n        if len(cluster_points) < 2:\n            continue\n        \n        chain = []\n        remaining_points = cluster_points.copy()\n        \n        start_point = remaining_points[np.argmin(remaining_points[:, 0])]\n        chain.append(start_point)\n        remaining_points = np.delete(remaining_points, \n                                   np.where((remaining_points == start_point).all(axis=1))[0], \n                                   axis=0)\n        \n        while len(remaining_points) > 0:\n            current = chain[-1]\n            distances = distance.cdist([current], remaining_points)\n            nearest_idx = np.argmin(distances)\n            nearest_point = remaining_points[nearest_idx]\n            \n            chain.append(nearest_point)\n            remaining_points = np.delete(remaining_points, nearest_idx, axis=0)\n        \n        chain = np.array(chain)\n        for i in range(len(chain) - 1):\n            pt1 = tuple(map(int, chain[i]))\n            pt2 = tuple(map(int, chain[i + 1]))\n            \n            cv2.line(mask_rgb, pt1, pt2, elegant_red, thickness=50)\n            \n            cv2.circle(mask_rgb, pt1, 8, elegant_red, -1)\n            cv2.circle(mask_rgb, pt2, 8, elegant_red, -1)\n    \n    mask_rgb = cv2.GaussianBlur(mask_rgb, (5, 5), 0)\n    \n    if visualization:\n        plt.figure(figsize=(12, 6))\n        \n        # Original binary mask\n        plt.subplot(121)\n        plt.imshow(image, cmap='gray')\n        plt.title('Original Image')\n        \n        # Elegant visualization\n        plt.subplot(122)\n        plt.imshow(cv2.cvtColor(mask_rgb, cv2.COLOR_BGR2RGB))\n        plt.title('Chain Cluster Visualization')\n        \n        plt.tight_layout()\n        plt.show()\n    \n    return mask_rgb\n\ndef enhanced_cluster_shapes(binary_mask, eps=50, min_samples=3):\n    contours, _ = cv2.findContours(binary_mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    centroids = []\n    for contour in contours:\n        M = cv2.moments(contour)\n        if M[\"m00\"] != 0:\n            cx = int(M[\"m10\"] / M[\"m00\"])\n            cy = int(M[\"m01\"] / M[\"m00\"])\n            centroids.append([cx, cy])\n    \n    if not centroids:\n        return [], []\n    \n    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(centroids)\n    labels = clustering.labels_\n    \n    return centroids, labels\n\ndef process_image_with_chain_mask(image):\n    binary = preprocess_image(image)\n    dragons_mask = filter_hemispheres(binary)\n    centroids, labels = enhanced_cluster_shapes(dragons_mask)\n    final_mask = create_chain_mask(image, dragons_mask, centroids, labels)\n    return final_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T00:18:17.009614Z","iopub.execute_input":"2025-01-21T00:18:17.009865Z","iopub.status.idle":"2025-01-21T00:18:17.029965Z","shell.execute_reply.started":"2025-01-21T00:18:17.009837Z","shell.execute_reply":"2025-01-21T00:18:17.029242Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = Image.open(\"/kaggle/input/3d-singleshot/3.png\").convert('RGB')\nimage = np.array(image)\n\nab = process_image_with_chain_mask(image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T00:18:17.403149Z","iopub.execute_input":"2025-01-21T00:18:17.403505Z","iopub.status.idle":"2025-01-21T00:18:18.580522Z","shell.execute_reply.started":"2025-01-21T00:18:17.403470Z","shell.execute_reply":"2025-01-21T00:18:18.579592Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Task 2","metadata":{}},{"cell_type":"code","source":"image = Image.open(\"/kaggle/input/3d-singleshot/2.png\").convert('RGB')\nimage = np.array(image)\n\nplt.imshow(image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T00:18:18.581548Z","iopub.execute_input":"2025-01-21T00:18:18.582068Z","iopub.status.idle":"2025-01-21T00:18:18.940387Z","shell.execute_reply.started":"2025-01-21T00:18:18.582023Z","shell.execute_reply":"2025-01-21T00:18:18.939581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install wget\n!pip -q install segment-anything","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T00:18:18.941386Z","iopub.execute_input":"2025-01-21T00:18:18.941676Z","iopub.status.idle":"2025-01-21T00:18:28.503438Z","shell.execute_reply.started":"2025-01-21T00:18:18.941652Z","shell.execute_reply":"2025-01-21T00:18:28.502233Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os\nimport wget\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n\nurl = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\ndestination_dir = \"sam-weights\"\ndestination_path = os.path.join(destination_dir, \"sam_vit_h_4b8939.pth\")\n\nif not os.path.exists(destination_dir):\n    os.makedirs(destination_dir)\n\nif not os.path.exists(destination_path):\n    wget.download(url, destination_path)\n    print(\"-- weights downloaded --\")\nelse:\n    print(\"-- weights already downloaded --\")\n\ndef load_sam(model_type, sam_checkpoint, device):\n    print(\"Loading model...\")\n    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n    print(f\"Shifting model to {device} device...\")\n    sam.to(device=device)\n    return sam\n\nmodel_type = \"vit_h\"\nsam_checkpoint = \"sam-weights/sam_vit_h_4b8939.pth\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nsam = load_sam(model_type, sam_checkpoint, device)\n\ndef get_sam_masks(image, sam_model):\n    print(\"Imstalling SAM mask generator...\")\n    mask_generator = SamAutomaticMaskGenerator(\n    model = sam_model,\n    points_per_side = 16,\n    pred_iou_thresh = 0.9,\n    crop_n_layers = 2,\n    crop_n_points_downscale_factor = 2,\n    min_mask_region_area = 200\n    )\n    print(\"Predicting masks...\")\n    masks = mask_generator.generate(image)\n    return masks\n\ndef show_anns(anns):\n    if len(anns) == 0:\n        return\n    if \"area\" in anns[0]:\n        sorted_anns = sorted(anns, key=lambda x: x['area'], reverse=True)\n    else:\n        sorted_anns = anns\n\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    # Initialize RGBA image (4 channels: R, G, B, A)\n    img = np.ones((sorted_anns[0]['segmentation'].shape[0], \n                   sorted_anns[0]['segmentation'].shape[1], 4))\n\n    # Set the alpha channel to 0 (transparent background)\n    img[:, :, 3] = 0\n\n    for ann in sorted_anns:\n        m = ann[\"segmentation\"]\n\n        # Ensure the mask is a boolean or binary array (True/False or 1/0)\n        if not m.dtype == bool:\n            m = m.astype(bool)\n\n        # Create a random color with transparency (R, G, B, Alpha)\n        color_mask = np.concatenate([np.random.random(3), [0.35]])\n\n        # Apply the color_mask only where the mask (m) is True\n        img[m] = color_mask  # This applies the color to areas where m is True\n\n    ax.imshow(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T00:18:28.504871Z","iopub.execute_input":"2025-01-21T00:18:28.505219Z","iopub.status.idle":"2025-01-21T00:18:55.863100Z","shell.execute_reply.started":"2025-01-21T00:18:28.505184Z","shell.execute_reply":"2025-01-21T00:18:55.862323Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef analyze_mask_contrast(image, mask):\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY).astype(float)\n    else:\n        gray = image.astype(float)\n    \n    mask_values = gray[mask]\n    \n    metrics = {\n        'min_val': np.min(mask_values),\n        'max_val': np.max(mask_values),\n        'contrast_range': np.max(mask_values) - np.min(mask_values),\n        'std_dev': np.std(mask_values),\n        'percentile_range': np.percentile(mask_values, 95) - np.percentile(mask_values, 5)\n    }\n    \n    hist, bins = np.histogram(mask_values, bins=50)\n    peaks = len([i for i in range(1, len(hist)-1) if hist[i] > hist[i-1] and hist[i] > hist[i+1]])\n    metrics['peak_count'] = peaks\n    \n    return metrics\n\ndef filter_curved_surfaces(image, masks):\n    mask_info = []\n    \n    for idx, mask_data in enumerate(masks):\n        mask = mask_data['segmentation']\n        if mask_data['area'] < 500: \n            continue\n            \n        metrics = analyze_mask_contrast(image, mask)\n        \n        contrast_score = (\n            0.4 * metrics['contrast_range'] + \n            0.3 * metrics['percentile_range'] + \n            0.3 * metrics['std_dev']  \n        )\n        \n        mask_info.append({\n            'index': idx,\n            'mask': mask,\n            'area': mask_data['area'],\n            'metrics': metrics,\n            'score': contrast_score\n        })\n    \n    sorted_masks = sorted(mask_info, key=lambda x: x['score'], reverse=True)\n    \n    threshold = np.mean([m['score'] for m in sorted_masks]) * 1.2\n    selected_masks = [m for m in sorted_masks if m['score'] > threshold]\n    \n    return selected_masks[:3]\n\ndef visualize_results(image, masks):\n    plt.figure(figsize=(20, 20))\n    \n    # 1. Первая строка: Original Image, Local Contrast Map, Intensity Histograms\n    plt.subplot(3, 3, 1)\n    plt.imshow(image)\n    plt.title('Original Image')\n    plt.axis('off')\n    \n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    contrast_map = np.zeros_like(gray, dtype=float)\n    window_size = 5\n    for i in range(window_size // 2, gray.shape[0] - window_size // 2):\n        for j in range(window_size // 2, gray.shape[1] - window_size // 2):\n            window = gray[i - window_size // 2:i + window_size // 2 + 1,\n                          j - window_size // 2:j + window_size // 2 + 1]\n            contrast_map[i, j] = np.max(window) - np.min(window)\n    \n    plt.subplot(3, 3, 2)\n    plt.imshow(contrast_map, cmap='hot')\n    plt.title('Local Contrast Map')\n    plt.axis('off')\n    \n    plt.subplot(3, 3, 3)\n    colors = ['r', 'g', 'b']\n    for idx, mask_info in enumerate(masks):\n        mask = mask_info['mask']\n        mask_values = gray[mask]\n        plt.hist(mask_values, bins=50, alpha=0.5, color=colors[idx], \n                 label=f'Surface {idx+1}\\nContrast: {mask_info[\"score\"]:.1f}')\n    plt.title('Intensity Histograms')\n    plt.legend()\n    plt.axis('on')\n    \n    # 2. Вторая строка: Surface 1, Surface 2, Surface 3\n    for idx, mask_info in enumerate(masks):\n        plt.subplot(3, 3, 4 + idx)\n        mask_vis = image.copy()\n        mask = mask_info['mask']\n        mask_vis[~mask] = mask_vis[~mask] * 0.3  # Затемняем фон\n        mask_vis[mask] = np.array([138, 18, 229])  # Закрашиваем маски заданным цветом\n        plt.imshow(mask_vis)\n        plt.title(f'Surface {idx+1}\\nContrast Score: {mask_info[\"score\"]:.1f}')\n        plt.axis('off')\n    \n    # 3. Третья строка: Combined Masks на оригинальном изображении\n    combined_image = image.copy()\n    overlay = np.zeros_like(image, dtype=np.float32)\n    color = np.array([138, 18, 229])  # Цвет масок\n    \n    for mask_info in masks:\n        mask = mask_info['mask']\n        overlay[mask] += color\n    \n    overlay = np.clip(overlay, 0, 255).astype(np.uint8)  # Ограничиваем значения\n    combined_image = cv2.addWeighted(image, 0.5, overlay, 0.5, 0)\n    \n    plt.subplot(3, 1, 3)  # Крупная визуализация внизу\n    plt.imshow(combined_image)\n    plt.title('Combined Masks on Original')\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return masks\n\ndef process_image(image, sam_masks):\n    selected_masks = filter_curved_surfaces(image, sam_masks)\n    result = visualize_results(image, selected_masks)\n    \"\"\"\n    print(\"\\nCurved surfaces with high contrast:\")\n    for idx, mask_info in enumerate(selected_masks):\n        metrics = mask_info['metrics']\n        print(f\"\\nSurface {idx+1}:\")\n        print(f\"Area: {mask_info['area']:.0f} pixels\")\n        print(f\"Contrast score: {mask_info['score']:.1f}\")\n        print(f\"Min-Max range: {metrics['contrast_range']:.1f}\")\n        print(f\"95th percentile range: {metrics['percentile_range']:.1f}\")\"\"\"\n    \n    return selected_masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T00:18:55.865117Z","iopub.execute_input":"2025-01-21T00:18:55.865461Z","iopub.status.idle":"2025-01-21T00:18:55.880320Z","shell.execute_reply.started":"2025-01-21T00:18:55.865441Z","shell.execute_reply":"2025-01-21T00:18:55.879614Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sam_masks = get_sam_masks(image, sam)\n\nselected_masks = process_image(image, sam_masks)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T00:18:55.881170Z","iopub.execute_input":"2025-01-21T00:18:55.881470Z","iopub.status.idle":"2025-01-21T00:19:54.608249Z","shell.execute_reply.started":"2025-01-21T00:18:55.881438Z","shell.execute_reply":"2025-01-21T00:19:54.607377Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Task 1","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom scipy.spatial import KDTree\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom collections import deque\nfrom typing import List, Tuple, Dict, Set\nfrom collections import defaultdict\n\ndef get_unique_pixels(image: np.ndarray) -> Dict[Tuple[int, int, int], int]:\n    pixels = image.reshape(-1, 3)\n    unique_pixels = defaultdict(int)\n    \n    for pixel in pixels:\n        unique_pixels[tuple(pixel)] += 1\n    \n    return dict(unique_pixels)\n\ndef validate_colors(image: np.ndarray, color_dict: Dict[str, List[int]]) -> None:\n    unique_pixels = get_unique_pixels(image)\n    valid_colors = {tuple(color) for color in color_dict.values()}\n    \n    print(\"Detected unique colors:\")\n    for color, count in unique_pixels.items():\n        status = \"✓\" if color in valid_colors else \"×\"\n        print(f\"{status} RGB{color}: {count} pixels\")\n        \n    invalid_colors = set(unique_pixels.keys()) - valid_colors\n    if invalid_colors:\n        print(\"\\nDetected unknown colors:\")\n        for color in invalid_colors:\n            print(f\"RGB{color}\")\n\ndef find_positions(image: np.ndarray, target_color: List[int]) -> List[Tuple[int, int]]:\n    positions = np.where(np.all(image == target_color, axis=2))\n    return list(zip(positions[0], positions[1]))\n\ndef get_neighbors(pos: Tuple[int, int], image: np.ndarray) -> List[Tuple[int, int]]:\n    y, x = pos\n    height, width = image.shape[:2]\n    neighbors = []\n    \n    for dy in [-1, 0, 1]:\n        for dx in [-1, 0, 1]:\n            if dy == 0 and dx == 0:\n                continue\n            new_y, new_x = y + dy, x + dx\n            if 0 <= new_y < height and 0 <= new_x < width:\n                neighbors.append((new_y, new_x))\n    \n    return neighbors\n\ndef map_to_closest_color(image: np.ndarray, color_dict: Dict[str, List[int]]) -> np.ndarray:\n    color_list = np.array(list(color_dict.values()))\n    tree = KDTree(color_list)\n    \n    reshaped_image = image.reshape(-1, 3)\n    \n    _, indices = tree.query(reshaped_image)\n    mapped_colors = color_list[indices]\n    \n    return mapped_colors.reshape(image.shape)\n\ndef visualize_image(original: np.ndarray, processed: np.ndarray) -> None:\n    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n    ax[0].imshow(original)\n    ax[0].set_title(\"Source Image\")\n    ax[0].axis(\"off\")\n    \n    ax[1].imshow(processed)\n    ax[1].set_title(\"Processed Image\")\n    ax[1].axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n\ndef get_terrain_type(color: np.ndarray, color_dict: Dict[str, List[int]]) -> str:\n    color_tuple = tuple(color)\n    for terrain, rgb in color_dict.items():\n        if color_tuple == tuple(rgb):\n            return terrain\n    return \"UNKNOWN\"\n\ndef is_valid_move(from_terrain: str, to_terrain: str, last_terrain: str) -> bool:\n    if to_terrain == 'ABYSS':\n        return False\n        \n    if to_terrain in ['START', 'END']:\n        return True\n        \n    if from_terrain in ['START', 'END']:\n        return to_terrain != 'ABYSS'\n        \n    if to_terrain == 'RAMP':\n        return True\n        \n    if from_terrain == 'RAMP':\n        return True\n        \n    if from_terrain == to_terrain:\n        return True\n        \n    if (from_terrain == 'SAND' and to_terrain == 'MOUNTAIN') or \\\n       (from_terrain == 'MOUNTAIN' and to_terrain == 'SAND'):\n        return last_terrain == 'RAMP'\n        \n    return False\n\ndef get_neighbors(pos: Tuple[int, int], image: np.ndarray) -> List[Tuple[int, int]]:\n    y, x = pos\n    height, width = image.shape[:2]\n    neighbors = []\n    \n    for dy, dx in [(0, 1), (1, 0), (0, -1), (-1, 0)]:\n        new_y, new_x = y + dy, x + dx\n        if 0 <= new_y < height and 0 <= new_x < width:\n            neighbors.append((new_y, new_x))\n    \n    return neighbors\n\ndef find_path(image: np.ndarray, color_dict: Dict[str, List[int]]) -> List[Tuple[int, int]]:\n    start_pos = find_positions(image, color_dict['START'])[0]\n    end_pos = find_positions(image, color_dict['END'])[0]\n    \n    height, width = image.shape[:2]\n    distances = np.full((height, width), -1, dtype=int)\n    distances[start_pos] = 0\n    \n    previous = {}\n    last_terrain = {}\n    \n    queue = deque([(start_pos, 'START', None)])\n    \n    while queue:\n        current_pos, current_terrain, prev_terrain = queue.popleft()\n        \n        if current_pos == end_pos:\n            break\n            \n        for next_pos in get_neighbors(current_pos, image):\n            if distances[next_pos] != -1: \n                continue\n                \n            next_terrain = get_terrain_type(image[next_pos], color_dict)\n            \n            if not is_valid_move(current_terrain, next_terrain, prev_terrain):\n                continue\n                \n            distances[next_pos] = distances[current_pos] + 1\n            previous[next_pos] = current_pos\n            last_terrain[next_pos] = current_terrain\n            queue.append((next_pos, next_terrain, current_terrain))\n    \n    if distances[end_pos] == -1:\n        raise ValueError(\"Path not found\")\n    \n    path = []\n    current = end_pos\n    while current is not None:\n        path.append(current)\n        current = previous.get(current)\n    \n    return path[::-1]\n\ndef visualize_path(image: np.ndarray, path: List[Tuple[int, int]], color_dict: Dict[str, List[int]], thickness: int = 7) -> np.ndarray:\n    result = image.copy()\n    height, width = image.shape[:2]\n    \n    for pos in path:\n        y, x = pos\n        for dy in range(-thickness//2, thickness//2 + 1):\n            for dx in range(-thickness//2, thickness//2 + 1):\n                if dy*dy + dx*dx <= (thickness//2)**2:\n                    new_y, new_x = y + dy, x + dx\n                    if 0 <= new_y < height and 0 <= new_x < width:\n                        result[new_y, new_x] = color_dict['PATH']\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T00:19:54.609222Z","iopub.execute_input":"2025-01-21T00:19:54.609463Z","iopub.status.idle":"2025-01-21T00:19:54.629522Z","shell.execute_reply.started":"2025-01-21T00:19:54.609441Z","shell.execute_reply":"2025-01-21T00:19:54.628628Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RAMP = [116, 116, 116]\nSAND = [147, 139, 101]\nMOUNTAIN = [94, 81, 22]\nABYSS = [0, 0, 0]\nSTART = [234, 51, 35]\nEND = [117, 251, 86]\nPATH = [138, 18, 229]\n\ncolor_dict = {\n    'RAMP': RAMP, \n    'SAND': SAND, \n    'MOUNTAIN': MOUNTAIN, \n    'ABYSS': ABYSS, \n    'START': START, \n    'END': END, \n    'PATH': PATH\n}\n\nimage = Image.open(\"/kaggle/input/3d-singleshot/1.png\").convert('RGB')\nimage = np.array(image)\n\nprocessed_image = map_to_closest_color(image, color_dict)\n\nvisualize_image(image, processed_image)\nvalidate_colors(processed_image, color_dict)\n\npath = find_path(processed_image, color_dict)\n\nresult_image = visualize_path(processed_image, path, {'PATH': PATH}, thickness=10)\n\nplt.imshow(result_image)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T00:20:10.946442Z","iopub.execute_input":"2025-01-21T00:20:10.946804Z","iopub.status.idle":"2025-01-21T00:20:23.212715Z","shell.execute_reply.started":"2025-01-21T00:20:10.946775Z","shell.execute_reply":"2025-01-21T00:20:23.211856Z"}},"outputs":[],"execution_count":null}]}